{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fd8281c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vc_clothes_dataset import VCClothesDataset, build_transforms\n",
    "from face_feature_extractor import FaceReIDModel\n",
    "from face_feature_extractor_wrapper import FaceReIDTrainWrapper\n",
    "from trainer import Trainer\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9814517",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = \"./data/vc_clothes\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 32\n",
    "LR = 0.00035\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5497508b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Załadowano zbiór VC-Clothes (train):\n",
      "  Liczba obrazów: 9449\n",
      "  Liczba unikalnych ID: 256\n",
      "  Liczba kamer: 4\n",
      "  Liczba unikalnych ubrań: 3\n",
      "Załadowano zbiór VC-Clothes (query):\n",
      "  Liczba obrazów: 1020\n",
      "  Liczba unikalnych ID: 256\n",
      "  Liczba kamer: 4\n",
      "  Liczba unikalnych ubrań: 3\n",
      "\n",
      "Przykładowy element batcha:\n",
      "Shape obrazów: torch.Size([32, 3, 256, 128])\n",
      "PIDs: tensor([202, 155,  57, 225, 143,  16, 150, 147, 201,  14,  49, 208, 130, 129,\n",
      "        163, 190,  50, 243, 192,  10, 127, 225, 176, 177, 200,  21, 214,  83,\n",
      "         49, 254, 184,  34])\n",
      "Clothes IDs: tensor([2, 1, 3, 1, 2, 1, 1, 1, 1, 1, 3, 2, 1, 3, 1, 1, 2, 1, 1, 1, 3, 1, 1, 1,\n",
      "        1, 3, 1, 1, 2, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "# Tworzenie transformacji\n",
    "train_transforms = build_transforms(is_train=True)\n",
    "val_transforms = build_transforms(is_train=False)\n",
    "\n",
    "# Instancja datasetu treningowego\n",
    "# VC-Clothes ma 9,449 obrazów treningowych [cite: 194]\n",
    "train_set = VCClothesDataset(DATA_ROOT, mode=\"train\", transform=train_transforms)\n",
    "train_loader = DataLoader(train_set, batch_size=32, shuffle=True, num_workers=4)\n",
    "\n",
    "# Instancja datasetu testowego (Query)\n",
    "# VC-Clothes ma 1,020 obrazów query [cite: 194]\n",
    "query_set = VCClothesDataset(DATA_ROOT, mode=\"query\", transform=val_transforms)\n",
    "test_loader = DataLoader(query_set, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "print(\"\\nPrzykładowy element batcha:\")\n",
    "imgs, pids, cams, clothes = next(iter(train_loader))\n",
    "print(f\"Shape obrazów: {imgs.shape}\")  # Oczekiwane [32, 3, 256, 128]\n",
    "print(f\"PIDs: {pids}\")\n",
    "print(f\"Clothes IDs: {clothes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a7f5a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = FaceReIDModel(device=DEVICE)\n",
    "train_model = FaceReIDTrainWrapper(base_model, num_classes=256)\n",
    "\n",
    "optimizer = optim.Adam(train_model.parameters(), lr=LR, weight_decay=5e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbf27583",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=train_model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=test_loader,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    device=DEVICE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e3eabef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rozpoczęcie treningu na urządzeniu: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoka 1:   0%|          | 0/296 [00:12<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "torch.cat(): expected a non-empty list of Tensors",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m results\n",
      "File \u001b[1;32mc:\\PW\\25Z\\GSN\\projekt\\trainer.py:163\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, num_epochs)\u001b[0m\n\u001b[0;32m    160\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, num_epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m--> 163\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    164\u001b[0m     val_loss, val_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidate(epoch)\n\u001b[0;32m    165\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend(train_loss, train_acc, val_loss, val_acc)\n",
      "File \u001b[1;32mc:\\PW\\25Z\\GSN\\projekt\\trainer.py:64\u001b[0m, in \u001b[0;36mTrainer.train_epoch\u001b[1;34m(self, epoch_idx)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# --- Forward pass ---\u001b[39;00m\n\u001b[1;32m---> 64\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m# --- Obsługa specyfiki FaceReIDModel ---\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m# Model może zwrócić krotkę (logits, valid_mask) jeśli używamy detekcji twarzy\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n",
      "File \u001b[1;32mc:\\Programing\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Programing\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\PW\\25Z\\GSN\\projekt\\face_feature_extractor_wrapper.py:51\u001b[0m, in \u001b[0;36mFaceReIDTrainWrapper.forward\u001b[1;34m(self, images)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# KROK 1: Detekcja i Preprocessing (na liście obrazów)\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Niestety MTCNN i preprocessing PIL działają na CPU i pojedynczych obrazach\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# W pętli treningowej \"online\" musimy to obsłużyć iteracyjnie.\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m images:\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;66;03m# Używamy logiki detekcji z oryginalnego modelu\u001b[39;00m\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;66;03m# Uwaga: w treningu zależy nam na czasie, można tu użyć detekcji na batchu jeśli MTCNN na to pozwala\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m     boxes, probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m     best_face \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m boxes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Programing\\Python\\Python312\\Lib\\site-packages\\facenet_pytorch\\models\\mtcnn.py:313\u001b[0m, in \u001b[0;36mMTCNN.detect\u001b[1;34m(self, img, landmarks)\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Detect all faces in PIL image and return bounding boxes and optional facial landmarks.\u001b[39;00m\n\u001b[0;32m    274\u001b[0m \n\u001b[0;32m    275\u001b[0m \u001b[38;5;124;03mThis method is used by the forward method and is also useful for face detection tasks\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;124;03m>>> img_draw.save('annotated_faces.png')\u001b[39;00m\n\u001b[0;32m    310\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 313\u001b[0m     batch_boxes, batch_points \u001b[38;5;241m=\u001b[39m \u001b[43mdetect_face\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmin_face_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43monet\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthresholds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfactor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    320\u001b[0m boxes, probs, points \u001b[38;5;241m=\u001b[39m [], [], []\n\u001b[0;32m    321\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m box, point \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(batch_boxes, batch_points):\n",
      "File \u001b[1;32mc:\\Programing\\Python\\Python312\\Lib\\site-packages\\facenet_pytorch\\models\\utils\\detect_face.py:83\u001b[0m, in \u001b[0;36mdetect_face\u001b[1;34m(imgs, minsize, pnet, rnet, onet, threshold, factor, device)\u001b[0m\n\u001b[0;32m     80\u001b[0m     scale_picks\u001b[38;5;241m.\u001b[39mappend(pick \u001b[38;5;241m+\u001b[39m offset)\n\u001b[0;32m     81\u001b[0m     offset \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m boxes_scale\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m---> 83\u001b[0m boxes \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mboxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     84\u001b[0m image_inds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(image_inds, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     86\u001b[0m scale_picks \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(scale_picks, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: torch.cat(): expected a non-empty list of Tensors"
     ]
    }
   ],
   "source": [
    "results = trainer.fit(num_epochs=EPOCHS)\n",
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
