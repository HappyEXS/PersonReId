{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "576a30e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "from torchvision import transforms\n",
    "import torchmetrics\n",
    "from facenet_pytorch import MTCNN\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2ec1800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "DATA_ROOT = \"./data/vc_clothes\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 32\n",
    "LR = 0.00035\n",
    "EPOCHS = 3\n",
    "\n",
    "print(f\"Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120d36b8",
   "metadata": {},
   "source": [
    "# Ekstaktor twarzy (mój Pyramidbox)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da2d1521",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceDetector:\n",
    "    def __init__(self, device=\"cpu\", confidence_threshold=0.8):\n",
    "        self.device = device\n",
    "        self.confidence_threshold = confidence_threshold\n",
    "\n",
    "        self.mtcnn = MTCNN(keep_all=False, device=device)\n",
    "\n",
    "        # Parametry z artykułu\n",
    "        self.expansion_pixels = 15\n",
    "        self.target_size = (50, 50)\n",
    "\n",
    "        self.transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize(self.target_size),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(\n",
    "                    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def get_face_tensor(self, image: Image):\n",
    "        boxes, probs = self.mtcnn.detect(image)\n",
    "\n",
    "        if boxes is None:\n",
    "            return None\n",
    "\n",
    "        # Filtrowanie po progu pewności\n",
    "        valid_indices = [\n",
    "            i for i, p in enumerate(probs) if p > self.confidence_threshold\n",
    "        ]\n",
    "\n",
    "        if not valid_indices:\n",
    "            return None\n",
    "\n",
    "        # Wybór najlepszej twarzy\n",
    "        best_idx = valid_indices[np.argmax(probs[valid_indices])]\n",
    "        box = boxes[best_idx]\n",
    "\n",
    "        # Wycinanie twarzy\n",
    "        x1, y1, x2, y2 = [int(b) for b in box]\n",
    "        w_img, h_img = image.size\n",
    "\n",
    "        x1 = max(0, x1 - self.expansion_pixels)\n",
    "        y1 = max(0, y1 - self.expansion_pixels)\n",
    "        x2 = min(w_img, x2 + self.expansion_pixels)\n",
    "        y2 = min(h_img, y2 + self.expansion_pixels)\n",
    "\n",
    "        face_crop = image.crop((x1, y1, x2, y2))\n",
    "        tensor = self.transform(face_crop)\n",
    "\n",
    "        return tensor.to(self.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45faa3b",
   "metadata": {},
   "source": [
    "# Dataset dla zbioru danych VC-Clothes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b822dfaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transforms(normalize=False, height=256, width=128):\n",
    "    normalize = transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "\n",
    "    if normalize:\n",
    "        transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize((height, width)),\n",
    "                transforms.ToTensor(),\n",
    "                normalize,\n",
    "            ]\n",
    "        )\n",
    "    else:\n",
    "        transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize((height, width)),\n",
    "                transforms.ToTensor(),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    return transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f791038",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VCClothesDataset(Dataset):\n",
    "    def __init__(self, root_dir, mode=\"train\", transform=None, verbose=True):\n",
    "\n",
    "        self.detector = FaceDetector(device=\"cpu\", confidence_threshold=0.5)\n",
    "        self.root_dir = root_dir\n",
    "        self.mode = mode\n",
    "        self.transform_img = (\n",
    "            build_transforms(normalize=True) if transform is None else transform\n",
    "        )\n",
    "\n",
    "        if mode == \"train\":\n",
    "            self.data_dir = root_dir + \"/train\"\n",
    "        elif mode == \"query\":\n",
    "            self.data_dir = root_dir + \"/query\"\n",
    "        elif mode == \"gallery\":\n",
    "            self.data_dir = root_dir + \"/gallery\"\n",
    "        else:\n",
    "            raise ValueError(\"Mode musi być jednym z: 'train', 'query', 'gallery'\")\n",
    "\n",
    "        if not os.path.exists(self.data_dir):\n",
    "            raise RuntimeError(f\"Katalog {self.data_dir} nie istnieje.\")\n",
    "\n",
    "        self.dataset = self._process_dir(self.data_dir)\n",
    "\n",
    "        self.img_paths = [x[0] for x in self.dataset]\n",
    "        self.pids = [x[1] for x in self.dataset]\n",
    "        self.camids = [x[2] for x in self.dataset]\n",
    "        self.clothes_ids = [x[3] for x in self.dataset]\n",
    "\n",
    "        # Zmiana mapowania PID dla treningu (muszą być ciągłe od 0 do N-1)\n",
    "        unique_pids = sorted(list(set(self.pids)))\n",
    "        self.pid_map = {pid: i for i, pid in enumerate(unique_pids)}\n",
    "        self.pids = [self.pid_map[pid] for pid in self.pids]\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Załadowano zbiór VC-Clothes ({mode}):\")\n",
    "            print(f\"  Liczba obrazów: {len(self.dataset)}\")\n",
    "            print(f\"  Liczba unikalnych ID: {len(set(self.pids))}\")\n",
    "            print(f\"  Liczba kamer: {len(set(self.camids))}\")\n",
    "            print(f\"  Liczba unikalnych ubrań: {len(set(self.clothes_ids))}\")\n",
    "\n",
    "    def _process_dir(self, dir_path):\n",
    "        img_paths = glob.glob(os.path.join(dir_path, \"*.jpg\")) + glob.glob(\n",
    "            os.path.join(dir_path, \"*.png\")\n",
    "        )\n",
    "\n",
    "        pattern = re.compile(\n",
    "            r\"([-\\d]+)-(\\d+)-(\\d+)-(\\d+)\"\n",
    "        )  # Format: PID-CAM-CLOTH_xxx.jpg\n",
    "\n",
    "        dataset = []\n",
    "        for img_path in img_paths:\n",
    "            filename = os.path.basename(img_path)\n",
    "\n",
    "            match = pattern.search(filename)\n",
    "            if not match:\n",
    "                continue\n",
    "\n",
    "            pid, camid, clothes_id, _ = map(int, match.groups())\n",
    "\n",
    "            if pid == -1:\n",
    "                continue  # Pomijanie obrazów \"junk\"\n",
    "\n",
    "            dataset.append((img_path, pid, camid, clothes_id))\n",
    "\n",
    "        return dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        path = self.img_paths[index]\n",
    "        pid = self.pids[index]\n",
    "        camid = self.camids[index]\n",
    "        clothes_id = self.clothes_ids[index]\n",
    "\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "\n",
    "        face = self.detector.get_face_tensor(img)\n",
    "\n",
    "        if self.transform_img is not None:\n",
    "            img = self.transform_img(img)\n",
    "\n",
    "        if isinstance(face, torch.Tensor):\n",
    "            face = face.float()\n",
    "        else:\n",
    "            face = torch.zeros((3, 50, 50), dtype=torch.float)\n",
    "\n",
    "        pid = torch.tensor(pid, dtype=torch.long)\n",
    "        camid = torch.tensor(camid, dtype=torch.long)\n",
    "        clothes_id = torch.tensor(clothes_id, dtype=torch.long)\n",
    "\n",
    "        return img, pid, camid, clothes_id, face"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3602039",
   "metadata": {},
   "source": [
    "# Moduł danych dla zbioru twarzy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a94ff93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VCClothesDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, root_dir, batch_size=32, num_workers=0, val_split: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.root_dir = root_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.val_split = val_split\n",
    "\n",
    "        self.transform = build_transforms(normalize=True)\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        gallery_dataset = VCClothesDataset(\n",
    "            root_dir=self.root_dir, mode=\"gallery\", transform=self.transform\n",
    "        )\n",
    "\n",
    "        total_size = len(gallery_dataset)\n",
    "        val_size = int(total_size * self.val_split)\n",
    "        train_size = total_size - val_size\n",
    "\n",
    "        self.train_ds, self.val_ds = torch.utils.data.random_split(\n",
    "            gallery_dataset, [train_size, val_size]\n",
    "        )\n",
    "\n",
    "        self.test_ds = VCClothesDataset(\n",
    "            root_dir=self.root_dir, mode=\"query\", transform=self.transform\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_ds,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.num_workers,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_ds,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test_ds,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928b30c9",
   "metadata": {},
   "source": [
    "# Face feature extractor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a79ccc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceFeatureExtractor(nn.Module):\n",
    "    def __init__(self, num_classes=256, embedding_dim=512):\n",
    "        super(FaceFeatureExtractor, self).__init__()\n",
    "\n",
    "        self.backbone = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.in_features = self.backbone.fc.in_features\n",
    "        self.backbone.fc = nn.Identity()\n",
    "\n",
    "        self.custom_head = nn.Sequential(\n",
    "            nn.Linear(self.in_features, embedding_dim),\n",
    "            nn.BatchNorm1d(embedding_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Linear(embedding_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.backbone.eval()\n",
    "\n",
    "        features = self.backbone(x)\n",
    "\n",
    "        embedding = self.custom_head(features)\n",
    "\n",
    "        embedding_norm = torch.nn.functional.normalize(embedding, p=2, dim=1)\n",
    "\n",
    "        return self.classifier(embedding_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8eac48e",
   "metadata": {},
   "source": [
    "# Moduł pytorch_lightning dla modelu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b25498d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceReIDModule(pl.LightningModule):\n",
    "    def __init__(self, num_classes=256, learning_rate=0.00035, embedding_dim=512):\n",
    "        super().__init__()\n",
    "        self.lr = learning_rate\n",
    "\n",
    "        self.model = FaceFeatureExtractor(\n",
    "            num_classes=num_classes, embedding_dim=embedding_dim\n",
    "        )\n",
    "\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        # self.accuracy = torchmetrics.Accuracy(\n",
    "        #     task=\"multiclass\", num_classes=num_classes\n",
    "        # )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        _, pids, _, _, faces = batch\n",
    "\n",
    "        # Filtorwanie\n",
    "        # valid_mask = faces.sum(dim=(1, 2, 3)) > 0\n",
    "\n",
    "        # if valid_mask.sum() == 0:\n",
    "        #     return None  # Pomijamy krok, jeśli w całym batchu nie ma twarzy\n",
    "\n",
    "        # valid_faces = faces[valid_mask]\n",
    "        # valid_pids = pids[valid_mask]\n",
    "\n",
    "        valid_faces = faces\n",
    "        valid_pids = pids\n",
    "\n",
    "        logits = self.model(valid_faces)\n",
    "        loss = self.criterion(logits, valid_pids)\n",
    "\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = (preds == valid_pids).float().mean()\n",
    "        # acc = self.accuracy(logits, valid_pids)\n",
    "\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"train_acc\", acc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        _, pids, _, _, faces = batch\n",
    "\n",
    "        # valid_mask = faces.sum(dim=(1, 2, 3)) > 0\n",
    "        # if valid_mask.sum() == 0:\n",
    "        #     return\n",
    "\n",
    "        # valid_faces = faces[valid_mask]\n",
    "        # valid_pids = pids[valid_mask]\n",
    "\n",
    "        valid_faces = faces\n",
    "        valid_pids = pids\n",
    "\n",
    "        logits = self.model(valid_faces)\n",
    "        loss = self.criterion(logits, valid_pids)\n",
    "\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = (preds == valid_pids).float().mean()\n",
    "\n",
    "        # acc = self.accuracy(logits, valid_pids)\n",
    "\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        self.log(\"val_acc\", acc, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        _, pids, _, _, faces = batch\n",
    "\n",
    "        # valid_mask = faces.sum(dim=(1, 2, 3)) > 0\n",
    "        # if valid_mask.sum() == 0:\n",
    "        #     return\n",
    "\n",
    "        # valid_faces = faces[valid_mask]\n",
    "        # valid_pids = pids[valid_mask]\n",
    "\n",
    "        valid_faces = faces\n",
    "        valid_pids = pids\n",
    "\n",
    "        logits = self.model(valid_faces)\n",
    "\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = (preds == valid_pids).float().mean()\n",
    "\n",
    "        # acc = self.accuracy(logits, valid_pids)\n",
    "\n",
    "        self.log(\"test_acc\", acc)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = Adam(\n",
    "            filter(lambda p: p.requires_grad, self.model.parameters()),\n",
    "            lr=self.lr,\n",
    "        )\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5177a9f5",
   "metadata": {},
   "source": [
    "# Trening\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bf7e279",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "c:\\Programing\\Python\\Python312\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\logger_connector\\logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n"
     ]
    }
   ],
   "source": [
    "dm = VCClothesDataModule(root_dir=DATA_ROOT, batch_size=64, num_workers=0)\n",
    "\n",
    "num_classes = 256\n",
    "\n",
    "model = FaceReIDModule(num_classes=num_classes, learning_rate=LR, embedding_dim=512)\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_acc\", mode=\"max\", filename=\"face-reid-{epoch:02d}-{val_acc:.2f}\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    max_epochs=20,\n",
    "    accelerator=\"auto\",  # Automatycznie wykryje GPU\n",
    "    devices=1,\n",
    "    callbacks=[checkpoint_callback],\n",
    "    # log_every_n_steps=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b61d164",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Programing\\Python\\Python312\\Lib\\site-packages\\facenet_pytorch\\models\\mtcnn.py:34: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(state_dict_path)\n",
      "c:\\Programing\\Python\\Python312\\Lib\\site-packages\\facenet_pytorch\\models\\mtcnn.py:79: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(state_dict_path)\n",
      "c:\\Programing\\Python\\Python312\\Lib\\site-packages\\facenet_pytorch\\models\\mtcnn.py:132: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(state_dict_path)\n",
      "\n",
      "  | Name      | Type                 | Params | Mode  | FLOPs\n",
      "-------------------------------------------------------------------\n",
      "0 | model     | FaceFeatureExtractor | 24.7 M | train | 0    \n",
      "1 | criterion | CrossEntropyLoss     | 0      | train | 0    \n",
      "-------------------------------------------------------------------\n",
      "1.2 M     Trainable params\n",
      "23.5 M    Non-trainable params\n",
      "24.7 M    Total params\n",
      "98.758    Total estimated model params size (MB)\n",
      "158       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "0         Total Flops\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Załadowano zbiór VC-Clothes (gallery):\n",
      "  Liczba obrazów: 8591\n",
      "  Liczba unikalnych ID: 256\n",
      "  Liczba kamer: 4\n",
      "  Liczba unikalnych ubrań: 3\n",
      "Załadowano zbiór VC-Clothes (query):\n",
      "  Liczba obrazów: 1020\n",
      "  Liczba unikalnych ID: 256\n",
      "  Liczba kamer: 4\n",
      "  Liczba unikalnych ubrań: 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4cc1b1b781948f593779dc225cfe5a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Programing\\Python\\Python312\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:434: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Programing\\Python\\Python312\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:434: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ded7359c189645058d229874b5d56d2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13143eb1f2ce47d5b5d6e32668b9d557",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "261a7d7715fd405b9dda771bb6698ff4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd007fe48a5245b2b1e3a61214153267",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "409480487b604228b28890294ebfa38d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6931c956ebe1436e969e228cfc400c9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e61d7c87c1b3486194c04a4c3247c153",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "830d18ca802e42b0bfe9843c46447654",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d27eaf0848e6459e90837ad434b4c12b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dd998037aa64a8d8a7ff2faa2b0422b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f222a1e69edb4aec8e29d351ae672c7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c8797f4f8394fd8b185e58a8aea1b57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c58e70ad4c8349fcb59f51602676ec18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66e8453cc08b49bab24904a206a8ceda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1236bc487e46428b8a334aa8dda342ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "967df01200e442728e24e9739f34f5be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d7af29581854ff5bd472d1f82c6d2c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d49e2db92574af682418eccc0ec8723",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b0c5ccb3bb1493397fde97b4cd654bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "222610797b004b74ae0fd529cd9e9fbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32a993315f7a4be393d7c8361981a5d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, datamodule=dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4021a1e",
   "metadata": {},
   "source": [
    "# Ewaluacja\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7824ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Załadowano zbiór VC-Clothes (gallery):\n",
      "  Liczba obrazów: 8591\n",
      "  Liczba unikalnych ID: 256\n",
      "  Liczba kamer: 4\n",
      "  Liczba unikalnych ubrań: 3\n",
      "Załadowano zbiór VC-Clothes (query):\n",
      "  Liczba obrazów: 1020\n",
      "  Liczba unikalnych ID: 256\n",
      "  Liczba kamer: 4\n",
      "  Liczba unikalnych ubrań: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Programing\\Python\\Python312\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:434: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "130c2a702a7d4541bd7ee11eef071bf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_acc             0.522549033164978\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_acc': 0.522549033164978}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(model, datamodule=dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "417d8fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classification_metrics(model, dataloader, device):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    correct_1 = 0\n",
    "    correct_5 = 0\n",
    "    correct_10 = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            _, pids, _, _, faces = batch\n",
    "\n",
    "            faces = faces.to(device)\n",
    "            pids = pids.to(device)\n",
    "\n",
    "            logits = model(faces)\n",
    "\n",
    "            _, pred_indices = logits.topk(k=10, dim=1)\n",
    "\n",
    "            pids_reshaped = pids.view(-1, 1)\n",
    "\n",
    "            matches = pred_indices == pids_reshaped\n",
    "\n",
    "            correct_1 += matches[:, 0].sum().item()\n",
    "            correct_5 += matches[:, :5].sum().item()\n",
    "            correct_10 += matches[:, :10].sum().item()\n",
    "\n",
    "            total += pids.size(0)\n",
    "\n",
    "    r1 = correct_1 / total\n",
    "    r5 = correct_5 / total\n",
    "    r10 = correct_10 / total\n",
    "\n",
    "    return r1, r5, r10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c579645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final evaluation on test set:\n",
      "Rank-1 (test_acc): 52.25%\n",
      "Rank-5  : 70.29%\n",
      "Rank-10 : 77.25%\n"
     ]
    }
   ],
   "source": [
    "r1, r5, r10 = evaluate_classification_metrics(model, dm.test_dataloader(), DEVICE)\n",
    "\n",
    "print(\"Final evaluation on test set:\")\n",
    "print(f\"Rank-1 (test_acc): {r1:.2%}\")\n",
    "print(f\"Rank-5  : {r5:.2%}\")\n",
    "print(f\"Rank-10 : {r10:.2%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
