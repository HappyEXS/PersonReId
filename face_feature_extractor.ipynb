{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "576a30e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "from torchvision import transforms\n",
    "import torchmetrics\n",
    "from facenet_pytorch import MTCNN\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2ec1800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "DATA_ROOT = \"./data/vc_clothes\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 32\n",
    "LR = 0.00035\n",
    "EPOCHS = 3\n",
    "\n",
    "print(f\"Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120d36b8",
   "metadata": {},
   "source": [
    "# Ekstaktor twarzy (mój Pyramidbox)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2d1521",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceDetector:\n",
    "    def __init__(self, device=\"cpu\", confidence_threshold=0.8):\n",
    "        self.device = device\n",
    "        self.confidence_threshold = confidence_threshold\n",
    "\n",
    "        self.mtcnn = MTCNN(keep_all=False, device=device)\n",
    "\n",
    "        # Parametry z artykułu\n",
    "        self.expansion_pixels = 15\n",
    "        self.target_size = (50, 50)\n",
    "\n",
    "        self.transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize(self.target_size),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(\n",
    "                    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def get_face_tensor(self, image: Image):\n",
    "        boxes, probs = self.mtcnn.detect(image)\n",
    "\n",
    "        if boxes is None:\n",
    "            return None\n",
    "\n",
    "        # Filtrowanie po progu pewności\n",
    "        valid_indices = [\n",
    "            i for i, p in enumerate(probs) if p > self.confidence_threshold\n",
    "        ]\n",
    "\n",
    "        if not valid_indices:\n",
    "            return None\n",
    "\n",
    "        # Wybór najlepszej twarzy\n",
    "        best_idx = valid_indices[np.argmax(probs[valid_indices])]\n",
    "        box = boxes[best_idx]\n",
    "\n",
    "        # Wycinanie twarzy\n",
    "        x1, y1, x2, y2 = [int(b) for b in box]\n",
    "        w_img, h_img = image.size\n",
    "\n",
    "        x1 = max(0, x1 - self.expansion_pixels)\n",
    "        y1 = max(0, y1 - self.expansion_pixels)\n",
    "        x2 = min(w_img, x2 + self.expansion_pixels)\n",
    "        y2 = min(h_img, y2 + self.expansion_pixels)\n",
    "\n",
    "        face_crop = image.crop((x1, y1, x2, y2))\n",
    "        tensor = self.transform(face_crop)\n",
    "\n",
    "        return tensor.to(self.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45faa3b",
   "metadata": {},
   "source": [
    "# Dataset dla zbioru danych VC-Clothes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b822dfaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transforms(normalize=False, height=256, width=128):\n",
    "    normalize = transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "\n",
    "    if normalize:\n",
    "        transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize((height, width)),\n",
    "                transforms.ToTensor(),\n",
    "                normalize,\n",
    "            ]\n",
    "        )\n",
    "    else:\n",
    "        transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize((height, width)),\n",
    "                transforms.ToTensor(),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    return transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f791038",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VCClothesDataset(Dataset):\n",
    "    def __init__(self, root_dir, mode=\"train\", transform=None, verbose=True):\n",
    "\n",
    "        self.detector = FaceDetector(device=\"cpu\", confidence_threshold=0.5)\n",
    "        self.root_dir = root_dir\n",
    "        self.mode = mode\n",
    "        self.transform_img = (\n",
    "            build_transforms(normalize=True) if transform is None else transform\n",
    "        )\n",
    "\n",
    "        if mode == \"train\":\n",
    "            self.data_dir = root_dir + \"/train\"\n",
    "        elif mode == \"query\":\n",
    "            self.data_dir = root_dir + \"/query\"\n",
    "        elif mode == \"gallery\":\n",
    "            self.data_dir = root_dir + \"/gallery\"\n",
    "        else:\n",
    "            raise ValueError(\"Mode musi być jednym z: 'train', 'query', 'gallery'\")\n",
    "\n",
    "        if not os.path.exists(self.data_dir):\n",
    "            raise RuntimeError(f\"Katalog {self.data_dir} nie istnieje.\")\n",
    "\n",
    "        self.dataset = self._process_dir(self.data_dir)\n",
    "\n",
    "        self.img_paths = [x[0] for x in self.dataset]\n",
    "        self.pids = [x[1] for x in self.dataset]\n",
    "        self.camids = [x[2] for x in self.dataset]\n",
    "        self.clothes_ids = [x[3] for x in self.dataset]\n",
    "\n",
    "        # Zmiana mapowania PID dla treningu (muszą być ciągłe od 0 do N-1)\n",
    "        unique_pids = sorted(list(set(self.pids)))\n",
    "        self.pid_map = {pid: i for i, pid in enumerate(unique_pids)}\n",
    "        self.pids = [self.pid_map[pid] for pid in self.pids]\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Załadowano zbiór VC-Clothes ({mode}):\")\n",
    "            print(f\"  Liczba obrazów: {len(self.dataset)}\")\n",
    "            print(f\"  Liczba unikalnych ID: {len(set(self.pids))}\")\n",
    "            print(f\"  Liczba kamer: {len(set(self.camids))}\")\n",
    "            print(f\"  Liczba unikalnych ubrań: {len(set(self.clothes_ids))}\")\n",
    "\n",
    "    def _process_dir(self, dir_path):\n",
    "        img_paths = glob.glob(os.path.join(dir_path, \"*.jpg\")) + glob.glob(\n",
    "            os.path.join(dir_path, \"*.png\")\n",
    "        )\n",
    "\n",
    "        pattern = re.compile(\n",
    "            r\"([-\\d]+)-(\\d+)-(\\d+)-(\\d+)\"\n",
    "        )  # Format: PID-CAM-CLOTH_xxx.jpg\n",
    "\n",
    "        dataset = []\n",
    "        for img_path in img_paths:\n",
    "            filename = os.path.basename(img_path)\n",
    "\n",
    "            match = pattern.search(filename)\n",
    "            if not match:\n",
    "                continue\n",
    "\n",
    "            pid, camid, clothes_id, _ = map(int, match.groups())\n",
    "\n",
    "            if pid == -1:\n",
    "                continue  # Pomijanie obrazów \"junk\"\n",
    "\n",
    "            dataset.append((img_path, pid, camid, clothes_id))\n",
    "\n",
    "        return dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        path = self.img_paths[index]\n",
    "        pid = self.pids[index]\n",
    "        camid = self.camids[index]\n",
    "        clothes_id = self.clothes_ids[index]\n",
    "\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "\n",
    "        face = self.detector.get_face_tensor(img)\n",
    "\n",
    "        if self.transform_img is not None:\n",
    "            img = self.transform_img(img)\n",
    "\n",
    "        if isinstance(face, torch.Tensor):\n",
    "            face = face.float()\n",
    "        else:\n",
    "            face = torch.zeros((3, 50, 50), dtype=torch.float)\n",
    "\n",
    "        pid = torch.tensor(pid, dtype=torch.long)\n",
    "        camid = torch.tensor(camid, dtype=torch.long)\n",
    "        clothes_id = torch.tensor(clothes_id, dtype=torch.long)\n",
    "\n",
    "        return img, pid, camid, clothes_id, face"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3602039",
   "metadata": {},
   "source": [
    "# Moduł danych dla zbioru twarzy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a94ff93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VCClothesDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, root_dir, batch_size=32, num_workers=0, val_split: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.root_dir = root_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.val_split = val_split\n",
    "\n",
    "        self.transform = build_transforms(normalize=True)\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        gallery_dataset = VCClothesDataset(\n",
    "            root_dir=self.root_dir, mode=\"gallery\", transform=self.transform\n",
    "        )\n",
    "\n",
    "        total_size = len(gallery_dataset)\n",
    "        val_size = int(total_size * self.val_split)\n",
    "        train_size = total_size - val_size\n",
    "\n",
    "        self.train_ds, self.val_ds = torch.utils.data.random_split(\n",
    "            gallery_dataset, [train_size, val_size]\n",
    "        )\n",
    "\n",
    "        self.test_ds = VCClothesDataset(\n",
    "            root_dir=self.root_dir, mode=\"query\", transform=self.transform\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_ds,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.num_workers,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_ds,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test_ds,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928b30c9",
   "metadata": {},
   "source": [
    "# Face feature extractor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a79ccc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceFeatureExtractor(nn.Module):\n",
    "    def __init__(self, num_classes=256, embedding_dim=512):\n",
    "        super(FaceFeatureExtractor, self).__init__()\n",
    "\n",
    "        self.backbone = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.in_features = self.backbone.fc.in_features\n",
    "        self.backbone.fc = nn.Identity()\n",
    "\n",
    "        self.custom_head = nn.Sequential(\n",
    "            nn.Linear(self.in_features, embedding_dim),\n",
    "            nn.BatchNorm1d(embedding_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Linear(embedding_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.backbone.eval()\n",
    "\n",
    "        features = self.backbone(x)\n",
    "\n",
    "        embedding = self.custom_head(features)\n",
    "\n",
    "        embedding_norm = torch.nn.functional.normalize(embedding, p=2, dim=1)\n",
    "\n",
    "        return self.classifier(embedding_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8eac48e",
   "metadata": {},
   "source": [
    "# Moduł pytorch_lightning dla modelu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b25498d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceReIDModule(pl.LightningModule):\n",
    "    def __init__(self, num_classes=256, learning_rate=0.00035, embedding_dim=512):\n",
    "        super().__init__()\n",
    "        self.lr = learning_rate\n",
    "\n",
    "        self.model = FaceFeatureExtractor(\n",
    "            num_classes=num_classes, embedding_dim=embedding_dim\n",
    "        )\n",
    "\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        # self.accuracy = torchmetrics.Accuracy(\n",
    "        #     task=\"multiclass\", num_classes=num_classes\n",
    "        # )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        _, pids, _, _, faces = batch\n",
    "\n",
    "        # Filtorwanie\n",
    "        # valid_mask = faces.sum(dim=(1, 2, 3)) > 0\n",
    "\n",
    "        # if valid_mask.sum() == 0:\n",
    "        #     return None  # Pomijamy krok, jeśli w całym batchu nie ma twarzy\n",
    "\n",
    "        # valid_faces = faces[valid_mask]\n",
    "        # valid_pids = pids[valid_mask]\n",
    "\n",
    "        valid_faces = faces\n",
    "        valid_pids = pids\n",
    "\n",
    "        logits = self.model(valid_faces)\n",
    "        loss = self.criterion(logits, valid_pids)\n",
    "\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = (preds == valid_pids).float().mean()\n",
    "        # acc = self.accuracy(logits, valid_pids)\n",
    "\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"train_acc\", acc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        _, pids, _, _, faces = batch\n",
    "\n",
    "        # valid_mask = faces.sum(dim=(1, 2, 3)) > 0\n",
    "        # if valid_mask.sum() == 0:\n",
    "        #     return\n",
    "\n",
    "        # valid_faces = faces[valid_mask]\n",
    "        # valid_pids = pids[valid_mask]\n",
    "\n",
    "        valid_faces = faces\n",
    "        valid_pids = pids\n",
    "\n",
    "        logits = self.model(valid_faces)\n",
    "        loss = self.criterion(logits, valid_pids)\n",
    "\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = (preds == valid_pids).float().mean()\n",
    "\n",
    "        # acc = self.accuracy(logits, valid_pids)\n",
    "\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        self.log(\"val_acc\", acc, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        _, pids, _, _, faces = batch\n",
    "\n",
    "        # valid_mask = faces.sum(dim=(1, 2, 3)) > 0\n",
    "        # if valid_mask.sum() == 0:\n",
    "        #     return\n",
    "\n",
    "        # valid_faces = faces[valid_mask]\n",
    "        # valid_pids = pids[valid_mask]\n",
    "\n",
    "        valid_faces = faces\n",
    "        valid_pids = pids\n",
    "\n",
    "        logits = self.model(valid_faces)\n",
    "\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = (preds == valid_pids).float().mean()\n",
    "\n",
    "        # acc = self.accuracy(logits, valid_pids)\n",
    "\n",
    "        self.log(\"test_acc\", acc)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = Adam(\n",
    "            filter(lambda p: p.requires_grad, self.model.parameters()),\n",
    "            lr=self.lr,\n",
    "        )\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5177a9f5",
   "metadata": {},
   "source": [
    "# Trening\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7bf7e279",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    }
   ],
   "source": [
    "dm = VCClothesDataModule(root_dir=DATA_ROOT, batch_size=BATCH_SIZE, num_workers=0)\n",
    "\n",
    "num_classes = 256\n",
    "\n",
    "model = FaceReIDModule(num_classes=num_classes, learning_rate=LR, embedding_dim=512)\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_acc\", mode=\"max\", filename=\"face-reid-{epoch:02d}-{val_acc:.2f}\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    max_epochs=10,\n",
    "    accelerator=\"auto\",  # Automatycznie wykryje GPU\n",
    "    devices=1,\n",
    "    callbacks=[checkpoint_callback],\n",
    "    # log_every_n_steps=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2b61d164",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name      | Type                 | Params | Mode  | FLOPs\n",
      "-------------------------------------------------------------------\n",
      "0 | model     | FaceFeatureExtractor | 24.7 M | train | 0    \n",
      "1 | criterion | CrossEntropyLoss     | 0      | train | 0    \n",
      "-------------------------------------------------------------------\n",
      "1.2 M     Trainable params\n",
      "23.5 M    Non-trainable params\n",
      "24.7 M    Total params\n",
      "98.758    Total estimated model params size (MB)\n",
      "158       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "0         Total Flops\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Załadowano zbiór VC-Clothes (gallery):\n",
      "  Liczba obrazów: 8591\n",
      "  Liczba unikalnych ID: 256\n",
      "  Liczba kamer: 4\n",
      "  Liczba unikalnych ubrań: 3\n",
      "Załadowano zbiór VC-Clothes (query):\n",
      "  Liczba obrazów: 1020\n",
      "  Liczba unikalnych ID: 256\n",
      "  Liczba kamer: 4\n",
      "  Liczba unikalnych ubrań: 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79846f7177f44a0bbcebbf39fb7d71c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "885564c520544292acfc64ae1f1987d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "678aa59a5f7c4e63b014bd3f17432157",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cf31c2b110146e1916ac82a345dccc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bce209b803c54247ae363929c786e28e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb363e3250614190b8abc9ac2978995b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20d80c46b8cf4b11adb83923ffd63031",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10d0909d46904ccc9aca785cc04acba8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cabe881cadd14265be9d568553050fed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f1dfc6eba504a3ea21a41733c4caa9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfb2c7735d4041d5a0b028951624d176",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3680a0b36f744f6288e4eadd5ce88725",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, datamodule=dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4021a1e",
   "metadata": {},
   "source": [
    "# Ewaluacja\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f7824ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Załadowano zbiór VC-Clothes (gallery):\n",
      "  Liczba obrazów: 8591\n",
      "  Liczba unikalnych ID: 256\n",
      "  Liczba kamer: 4\n",
      "  Liczba unikalnych ubrań: 3\n",
      "Załadowano zbiór VC-Clothes (query):\n",
      "  Liczba obrazów: 1020\n",
      "  Liczba unikalnych ID: 256\n",
      "  Liczba kamer: 4\n",
      "  Liczba unikalnych ubrań: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Programing\\Python\\Python312\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:434: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3d51bbb62234b259348e2916e646f24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_acc            0.45686274766921997\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_acc': 0.45686274766921997}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(model, datamodule=dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "417d8fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classification_metrics(model, dataloader, device):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    correct_1 = 0\n",
    "    correct_5 = 0\n",
    "    correct_10 = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            _, pids, _, _, faces = batch\n",
    "\n",
    "            faces = faces.to(device)\n",
    "            pids = pids.to(device)\n",
    "\n",
    "            logits = model(faces)\n",
    "\n",
    "            _, pred_indices = logits.topk(k=10, dim=1)\n",
    "\n",
    "            pids_reshaped = pids.view(-1, 1)\n",
    "\n",
    "            matches = pred_indices == pids_reshaped\n",
    "\n",
    "            correct_1 += matches[:, 0].sum().item()\n",
    "            correct_5 += matches[:, :5].sum().item()\n",
    "            correct_10 += matches[:, :10].sum().item()\n",
    "\n",
    "            total += pids.size(0)\n",
    "\n",
    "    r1 = correct_1 / total\n",
    "    r5 = correct_5 / total\n",
    "    r10 = correct_10 / total\n",
    "\n",
    "    return r1, r5, r10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3c579645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final evaluation on test set:\n",
      "Rank-1 (test_acc): 45.69%\n",
      "Rank-5  : 68.92%\n",
      "Rank-10 : 75.69%\n"
     ]
    }
   ],
   "source": [
    "r1, r5, r10 = evaluate_classification_metrics(model, dm.test_dataloader(), DEVICE)\n",
    "\n",
    "print(\"Final evaluation on test set:\")\n",
    "print(f\"Rank-1 (test_acc): {r1:.2%}\")\n",
    "print(f\"Rank-5  : {r5:.2%}\")\n",
    "print(f\"Rank-10 : {r10:.2%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
